## Classical ML
Simple models win when problem structure is clear and data is tidy. Linear models, decision trees, and ensembles offer interpretability, fast iteration, and modest compute demands. They encourage discipline: feature engineering forces understanding of signals and their failure cases.

Weaknesses show up with messy, high-dimensional data and shifting domains. Manual features can ossify assumptions. However, the transparency of classical methods is valuable during incident response: you can trace which feature drove a decision and adjust quickly.

Use classical ML when the decision surface is stable, the cost of error is legible, and you want governance to be straightforward. Resist abandoning them for neural networks purely for prestige; complexity should be earned by necessity.
